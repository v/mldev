{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - tensors tutorial\n",
    "\n",
    "Contents are taken from:\n",
    "\n",
    "[Pytorch - Intro to tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "\n",
    "[Pytorch - Another intro to tensors](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensors from an array is pretty straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data=tensor([[1, 2],\n",
      "        [3, 4]]) x_data.dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    [1,2],\n",
    "    [3,4],\n",
    "]\n",
    "\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"{x_data=} {x_data.dtype=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have data types and it's important to keep track of them for neural nets to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data=tensor([[1., 2.],\n",
      "        [3., 4.]]) x_data.dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.Tensor(data)\n",
    "print(f\"{x_data=} {x_data.dtype=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` always creates a tensor of type float32\n",
    "\n",
    "`torch.tensor` creates a tensor where the data type is automatically inferred (can be an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npa = np.array(data)\n",
    "torch.from_numpy(npa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8694, 0.4451],\n",
       "        [0.3105, 0.8647]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rand = torch.rand_like(x_data)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.ones_like` retains the shape and data type of another tensor but makes a tensor of 1 values.\n",
    "\n",
    "`torch.rand_like` retains the shape and data type of another tensor but makes a tensor with uniform random values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor=tensor([[0.6626, 0.9979, 0.5738],\n",
      "        [0.8015, 0.2067, 0.0500]])\n",
      "ones_tensor=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zeros_tensor=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"{rand_tensor=}\")\n",
    "print(f\"{ones_tensor=}\")\n",
    "print(f\"{zeros_tensor=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are also stored on a device (cpu / gpu).\n",
    "\n",
    "They have to explicitly be moved from CPU -> GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((3, 4))\n",
    "\n",
    "print(tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing\n",
    "\n",
    "There are many ways to select elements from a tensor.\n",
    "Generally, the indexing rules are similar to Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor=tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "FIRST ROW:\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3])\n",
      "FIRST COL:\n",
      "tensor([0, 4, 8])\n",
      "LAST COL:\n",
      "tensor([ 3,  7, 11])\n",
      "tensor([ 3,  7, 11])\n",
      "EVERY EVEN COL:\n",
      "tensor([[ 0,  2],\n",
      "        [ 4,  6],\n",
      "        [ 8, 10]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(12).reshape(3, 4)\n",
    "\n",
    "print(f\"{tensor=}\")\n",
    "\n",
    "# print the first row\n",
    "print(\"FIRST ROW:\")\n",
    "print(tensor[0])\n",
    "print(tensor[0, :])\n",
    "print(tensor[0, ...])\n",
    "\n",
    "# print the first column\n",
    "print(\"FIRST COL:\")\n",
    "print(tensor[:, 0])\n",
    "\n",
    "# print the last column\n",
    "print(\"LAST COL:\")\n",
    "print(tensor[:, -1])\n",
    "print(tensor[..., -1])\n",
    "\n",
    "# print every even column\n",
    "print(\"EVERY EVEN COL:\")\n",
    "print(tensor[:, ::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  2,  3],\n",
       "        [ 4,  0,  6,  7],\n",
       "        [ 8,  0, 10, 11]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify a column\n",
    "tensor[:, 1] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining tensors with `cat` and `stack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.shape=torch.Size([3, 4])\n",
      "t1.shape=torch.Size([3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat\n",
    "\n",
    "print(f\"{tensor.shape=}\")\n",
    "t1 = torch.cat([tensor, tensor, tensor, tensor], dim=1)\n",
    "print(f\"{t1.shape=}\")\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2.shape=torch.Size([2, 9, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = torch.ones((2, 3, 4))\n",
    "t2 = torch.cat([input2, input2, input2], dim=1)\n",
    "print(f\"{t2.shape=}\")\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1.shape=torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack\n",
    "\n",
    "tensor = torch.ones((3, 4))\n",
    "t1 = torch.stack([tensor, tensor], dim=0)\n",
    "print(f\"{t1.shape=}\")\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.stack` is different from `torch.cat` \n",
    "\n",
    "`torch.stack` creates a new tensor, but the inputs are glued together along a new dimension (that you specify).\n",
    "`torch.cat` creates a new tensor, but the inputs are glued together along an existing dimension (that you specify).\n",
    "\n",
    "concatenating two 3x4 tensors, gives you a new tensor of size 3x8\n",
    "stacking two 3x4 tensors, gives you a new tensor of size 2x3x4\n",
    "\n",
    "[Reference explanation on stackoverflow](https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randn vs rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2626, -0.3817,  1.3986, -0.6001],\n",
       "         [-0.7493,  0.8892,  0.0382, -0.9166],\n",
       "         [-0.9668, -0.2828, -0.6239,  0.2364]],\n",
       "\n",
       "        [[ 0.2725, -0.8735,  0.5722, -1.0876],\n",
       "         [ 0.6604, -0.8016,  0.4640,  1.4917],\n",
       "         [-1.1043, -1.4910,  0.1564, -0.7772]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn` and `torch.rand` are also different.\n",
    "\n",
    "`rand` samples uniformly between [0, 1]\n",
    "\n",
    "`randn` samples with a normal distribution where the mean is 0 and the variance is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x11f553d00>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float32)\n",
    "y = torch.tensor([4, 5, 6], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you perform operations on tensors, Pytorch remembers how each value was produced.\n",
    "\n",
    "Here `z.grad_fn=AddBackward0` is telling us that `z` was produced through an addition of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x11f069180>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `s.grad_fn=SumBackward0` is telling us that `s` was produced by summing up the values of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A complex auto differentiation code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "Before we require gradients, z.grad_fn is None\n",
      "After we require gradients, z.grad_fn is <AddBackward0 object at 0x110c0fbe0>\n",
      "z.requires_grad=True\n",
      "After detatching new_z.grad_fn=None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(\"Before we require gradients, z.grad_fn is\", z.grad_fn)\n",
    "\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(\"After we require gradients, z.grad_fn is\", z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(f\"{z.requires_grad=}\")\n",
    "\n",
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(f\"After detatching {new_z.grad_fn=}\")\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## view and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(2, 3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `reshape` and `view` return a tensor with the same contents as the input but with a new shape.\n",
    "\n",
    "The difference between `reshape` and `reshape` is not so obvious.\n",
    "\n",
    "`view` always returns a view into the existing tensor. So when you modify the result, the original tensor gets modified too.\n",
    "\n",
    "`reshape` tries to return a view if it can, but if it can't, it makes a copy of the input tensor and reshapes that.\n",
    "\n",
    "[This thread](https://discuss.pytorch.org/t/difference-between-view-reshape-and-permute/54157/2) is a good explanation of the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_contiguous()=True\n",
      "b.is_contiguous()=False\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79])\n",
      "torch.Size([2, 4, 10])\n",
      ".view() operation failed: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(80).view(4, 10, 2)\n",
    "b = a.permute(2, 0, 1)\n",
    "\n",
    "print(f\"{a.is_contiguous()=}\")\n",
    "print(f\"{b.is_contiguous()=}\")\n",
    "\n",
    "print(a.view(-1))\n",
    "print(b.shape)\n",
    "\n",
    "try:\n",
    "    print(b.view(1, 80)) # this fails\n",
    "except Exception as e:\n",
    "    print(\".view() operation failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are tensors stored in memory?\n",
    "\n",
    "[Computerphile has a good explanation of how images are stored in memory](https://www.youtube.com/watch?v=06OHflWNCOE).\n",
    "\n",
    "[And how n dimensional tensors are stored in memory](https://www.youtube.com/watch?v=DfK83xEtJ_k).\n",
    "\n",
    "You can print the stride and size of a pytorch tensor to see this in action.\n",
    "\n",
    "Stride is the number of bytes from one element from this dimension to another element of this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size()=torch.Size([4, 10, 2])\n",
      "a.stride()=(20, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(80).view(4, 10, 2)\n",
    "\n",
    "print(f\"{a.size()=}\")\n",
    "print(f\"{a.stride()=}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7feba0c39194aeb14b5e7bd5560b8c45dc2d3c126975e5595ee228f6b74cec04"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
