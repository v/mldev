{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing broadcasting rules to see why keepdim=True matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([\n",
    "    [1, 2], \n",
    "    [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.],\n",
      "        [7.]])\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.3333, 0.6667],\n",
      "        [0.4286, 0.5714]])\n"
     ]
    }
   ],
   "source": [
    "s1 = a.sum(1, True)\n",
    "print(s1)\n",
    "print(s1.shape)\n",
    "\n",
    "print(a / s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 7.])\n",
      "torch.Size([2])\n",
      "tensor([[0.3333, 0.2857],\n",
      "        [1.0000, 0.5714]])\n"
     ]
    }
   ],
   "source": [
    "s2 = a.sum(1, False)\n",
    "print(s2)\n",
    "print(s2.shape)\n",
    "\n",
    "print(a / s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `a` is a 2x2 tensor.\n",
    "\n",
    "`s1 = a.sum(1, keepdim=True)` gives a tensor with shape `[2, 1]`\n",
    "\n",
    "`s2 = a.sum(1, keepdim=False)` gives a tensor with shape `[2]`\n",
    "\n",
    "`a / s1` causes each column to be copied. `[[1, 2], [3, 4]]` is divided by `[[3, 3], [7, 7]]`\n",
    "`a / s2` causes each row to be copied. `[[1, 2], [3, 4]]` is divided by `[[3, 7], [3, 7]]`.\n",
    "\n",
    "Another way to say this is that a tensor with one dimension is treated like a row tensor. When you perform a broadcasting operation with a one dimensional tensor, you will treat the one dimensional tensor as a row and make copies of it to make multiple columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing broadcasting rules with a 3x2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = torch.Tensor([\n",
    "    [1, 2], \n",
    "    [3, 4],\n",
    "    [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[0.3333, 0.6667],\n",
      "        [0.4286, 0.5714],\n",
      "        [0.4545, 0.5455]])\n"
     ]
    }
   ],
   "source": [
    "s1 = a2.sum(1, True)\n",
    "print(s1.shape)\n",
    "\n",
    "print(a2 / s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m s2 \u001b[38;5;241m=\u001b[39m a2\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(s2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "s2 = a2.sum(1, False)\n",
    "print(s2.shape)\n",
    "\n",
    "print(a2 / s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 3x2, tensor, we see that the broadcasting operation fails entirely, because we match the trailing dimension before moving on to other dimensions.\n",
    "\n",
    "This also highlights that the overall operation we are trying to do doesn't make sense. If we trying to turn each row from a list of counts into a list of probabilities, the operation will fail entirely, rather than succeed buggily when we use a non-square tensor."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7feba0c39194aeb14b5e7bd5560b8c45dc2d3c126975e5595ee228f6b74cec04"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
